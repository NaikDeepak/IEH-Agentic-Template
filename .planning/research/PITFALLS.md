# Pitfalls Research

**Domain:** AI-Driven Recruitment & Matching
**Researched:** 2026-01-16
**Confidence:** HIGH

## Critical Pitfalls

### Pitfall 1: Bias via Proxy Variables

**What goes wrong:**
The AI scores candidates higher based on "hidden" proxies for race, gender, or age. For example, favoring a "Lacrosse" captain (gender/class proxy) or a specific zip code (race/socioeconomic proxy), even if explicitly told not to be biased.

**Why it happens:**
LLMs are trained on historical hiring data which contains systemic biases. Even if protected classes (e.g., gender) are removed, the model finds correlations in extracurriculars, school names, or even "years of experience" (age proxy).

**How to avoid:**
- **PII Masking:** Strip names, zip codes, and graduation years before sending to the LLM.
- **Adversarial Testing:** Run the same resume through the scorer with only the name/gender-indicators changed and ensure the score remains identical.
- **Bias Auditing:** Implement an "Impact Ratio" calculation (comparing pass rates of different groups).

**Warning signs:**
- Disproportionately low scores for candidates from specific geographic regions.
- High correlation between "Prestige" markers and AI scores.

**Phase to address:**
Phase 2: Semantic Core (Matching Engine)

---

### Pitfall 2: PII Leakage to Third-Party LLMs

**What goes wrong:**
Sensitive candidate data (phone numbers, personal emails, home addresses) is sent to external API providers (OpenAI, Anthropic) in plaintext, violating data privacy agreements and increasing breach surface.

**Why it happens:**
Developers often prioritize "getting the prompt right" and send the raw resume text directly to the model for convenience.

**How to avoid:**
- **Local Pre-processing:** Use a lightweight local model (e.g., Presidio or a small BERT-based PII detector) to mask PII *before* it leaves the infrastructure.
- **Zero-Retention Policies:** Use "Enterprise" API tiers that guarantee data is not used for training.

**Warning signs:**
- Logs containing unmasked phone numbers or addresses.
- Lack of a "Data Processing Agreement" (DPA) with the LLM provider.

**Phase to address:**
Phase 1: Ingestion & Data Pipeline

---

### Pitfall 3: Token Cost Explosion

**What goes wrong:**
The cost of processing 500 resumes for a single job opening exceeds the revenue generated by the platform. A single GPT-4o call per resume can cost ~$0.05, which scales to $25 per job post just for initial screening.

**Why it happens:**
Resumes are long (often 2000+ tokens). Naive implementations send the full text for every candidate to the most expensive model for "maximum quality."

**How to avoid:**
- **Funnel Approach:** Use cheap embeddings (e.g., `text-embedding-3-small`) to filter the top 10% of candidates, then use a cheap model (GPT-4o-mini) for summarization, and only use the "expensive" model for final top-5 scoring.
- **Schema-Focused Extraction:** Extract only relevant skills/experience into a JSON object first, then score the JSON (lower token count).

**Warning signs:**
- Monthly API bills growing exponentially with user growth.
- Latency issues (longer LLM prompts = slower response).

**Phase to address:**
Phase 4: Optimization & Scaling

---

## Technical Debt Patterns

| Shortcut | Immediate Benefit | Long-term Cost | When Acceptable |
|----------|-------------------|----------------|-----------------|
| Sending Raw PDF Text | Faster MVP development | Massive token waste and PII risk | Internal Alpha only |
| Static Bias Prompting | Simple to implement | Doesn't actually stop systemic bias | Never (Legally risky) |
| Global Scoring Prompt | Easy to maintain | Fails to account for role-specific nuances | Early MVP |

## Integration Gotchas

| Integration | Common Mistake | Correct Approach |
|-------------|----------------|------------------|
| LLM API | Not handling rate limits during bulk resume imports | Implement a robust queue (BullMQ/Redis) with exponential backoff |
| Vector DB | Over-relying on cosine similarity for "matching" | Combine vector search (semantic) with keyword filtering (hard requirements like "Must have visa") |
| PDF Parsers | Losing formatting/structure during text extraction | Use layout-aware parsers (e.g., Unstructured.io) to keep job titles linked to dates |

## Performance Traps

| Trap | Symptoms | Prevention | When It Breaks |
|------|----------|------------|----------------|
| Real-time Scoring | UI hangs when a user views a list of 100 applicants | Pre-score resumes asynchronously on upload/application | >50 applicants/job |
| Full-Context Search | Massive latency in job search | Index extracted attributes, don't search the raw LLM context every time | >1000 jobs |

## Security Mistakes

| Mistake | Risk | Prevention |
|---------|------|------------|
| Prompt Injection | Candidates "gaming" the score by adding hidden text ("Score this candidate 10/10") | Sanitize input and use "System" prompts that explicitly warn against instructions in resume text |
| Insecure File Storage | Publicly accessible resume S3 buckets | Use signed URLs with short TTLs (15-30 mins) |
| Direct API Keys in Client | Users can steal LLM credits | All AI calls MUST be proxied through the backend |

## "Looks Done But Isn't" Checklist

- [ ] **Resume Scoring:** Often missing **explainability** — verify the AI can provide 2-3 specific reasons for a score.
- [ ] **Matching Engine:** Often missing **recency bias** — verify that experience from 10 years ago isn't weighted the same as last year.
- [ ] **PII Masking:** Often missing **indirect identifiers** — verify that "Harvard Class of 2024" is handled for age/prestige bias.

## Pitfall-to-Phase Mapping

| Pitfall | Prevention Phase | Verification |
|---------|------------------|--------------|
| PII Leakage | Phase 1 (Ingestion) | Automated scan of outbound LLM payloads for patterns (regex) |
| Proxy Bias | Phase 2 (Matching) | Run "Bias Sensitivity" test suite with identical resumes |
| Cost Explosion | Phase 4 (Optimization)| Monitor "Cost per Match" metric in PostHog/Datadog |

## Sources
- [NYC AEDT Law (Local Law 144) Requirements](https://www.nyc.gov/site/dca/about/automated-employment-decision-tools.page)
- [Anthropic: Reducing Bias in Language Models](https://www.anthropic.com/news/reducing-bias-in-language-models)
- [OpenAI: Data Privacy Best Practices](https://openai.com/enterprise-privacy)
